{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import psycopg2\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature file names\n",
    "feature_files = ['admission_type', 'age', 'aids_haem_mets', 'bicarb', 'bilirubin', 'blood_pressure', 'fio2', \\\n",
    "            'gcs_eyes', 'gcs_motor', 'gcs_verbal', 'heart_rate', 'pao2', 'potassium', 'sodium', 'temperature', 'urea', 'urine', 'wbc']\n",
    "\n",
    "# Define processed feature names\n",
    "feature_names = ['admission_type', 'age', 'aids_haem_mets', 'bicarb_24h', 'bilirubin_24h', 'bp_24h', 'fio2_24h', \\\n",
    "            'gcs_eyes_24h', 'gcs_motor_24h', 'gcs_verbal_24h', 'hr_24h', 'pao2_24h', 'potassium_24h', 'sodium_24h', 'temp_24h', 'urea_24h', 'urine_24h', 'wbc_24h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "for i in range(len(feature_files)):\n",
    "    _data = np.load('res/{}.npy'.format(feature_files[i]), allow_pickle=True).tolist()\n",
    "    exec(\"{} = _data['{}']\".format(feature_names[i], feature_names[i]))\n",
    "print(\"All files loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of files\n",
    "for i in range(len(feature_files)):\n",
    "    print(\"{}: \".format(feature_names[i]))\n",
    "    exec(\"shape = np.shape({})\\nprint(shape)\".format(feature_names[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast static features so they have 24 time steps\n",
    "sched_surg_24h = np.column_stack((admission_type[:,0], np.tile(admission_type[:,1], (24,1)).T))\n",
    "unsched_surg_24h = np.column_stack((admission_type[:,0], np.tile(admission_type[:,2], (24,1)).T))\n",
    "medical_24h = np.column_stack((admission_type[:,0], np.tile(admission_type[:,3], (24,1)).T))\n",
    "age_24h = np.column_stack((age[:,0], np.tile(age[:,1], (24,1)).T))\n",
    "aids_24h = np.column_stack((aids_haem_mets[:,0], np.tile(aids_haem_mets[:,1], (24,1)).T))\n",
    "haem_24h = np.column_stack((aids_haem_mets[:,0], np.tile(aids_haem_mets[:,2], (24,1)).T))\n",
    "mets_24h = np.column_stack((aids_haem_mets[:,0], np.tile(aids_haem_mets[:,3], (24,1)).T))\n",
    "\n",
    "# Check the dimensions are correct\n",
    "print(\"sched_surg_24h: \", np.shape(sched_surg_24h))\n",
    "print(\"unsched_surg_24h: \", np.shape(unsched_surg_24h))\n",
    "print(\"medical_24h: \", np.shape(medical_24h))\n",
    "print(\"age_24h: \", np.shape(age_24h))\n",
    "print(\"aids_24h: \", np.shape(aids_24h))\n",
    "print(\"haem_24h: \", np.shape(haem_24h))\n",
    "print(\"mets_24h: \", np.shape(mets_24h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all features into one array\n",
    "dataset = np.dstack((sched_surg_24h, unsched_surg_24h, medical_24h, age_24h, aids_24h, haem_24h, mets_24h, bicarb_24h, bilirubin_24h, bp_24h, \\\n",
    "                   fio2_24h, gcs_eyes_24h, gcs_motor_24h, gcs_verbal_24h, hr_24h, pao2_24h, potassium_24h, sodium_24h, temp_24h, urea_24h, urine_24h, wbc_24h))\n",
    "\n",
    "# Remove the patient IDs\n",
    "dataset = dataset[:,1:,:]\n",
    "\n",
    "# Transpose dataset so it is in the shape (m,n,T)\n",
    "dataset = np.transpose(dataset, (0,2,1))\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"dataset: \", np.shape(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data to make sure there's nothing too crazy going on\n",
    "feat_names = [\"sched_surg_24h\", \"unsched_surg_24h\", \"medical_24h\", \"age_24h\", \"aids_24h\", \"haem_24h\", \"mets_24h\", \"bicarb_24h\", \"bilirubin_24h\", \"bp_24h\", \\\n",
    "            \"fio2_24h\", \"gcs_eyes_24h\", \"gcs_motor_24h\", \"gcs_verbal_24h\", \"hr_24h\", \"pao2_24h\", \"potassium_24h\", \"sodium_24h\", \"temp_24h\", \"urea_24h\", \"urine_24h\", \"wbc_24h\"]\n",
    "\n",
    "n = len(feat_names)\n",
    "for i in range(n):\n",
    "    feature = dataset[:,i,:]\n",
    "    print(\"{}:\".format(feat_names[i]))\n",
    "    print(\"Min = {}; Max = {}; Mean = {}; Standard Deviation = {}\".format(round(np.min(feature),2), round(np.max(feature),2), round(np.mean(feature),2), round(np.std(feature),2)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen above, some features have values below zero and need to be clipped\n",
    "clipped_dataset = np.clip(dataset, 0, a_max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mortality data\n",
    "_data = np.load('res/mortality.npy', allow_pickle=True).tolist()\n",
    "mortality = _data['mortality']\n",
    "\n",
    "# Remove the patient IDs\n",
    "mortality = mortality[:,1]\n",
    "\n",
    "# Check the shape of the mortality\n",
    "print(\"mortality: \", np.shape(mortality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(clipped_dataset, mortality, train_size=0.8, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=0.5, random_state=42)\n",
    "\n",
    "# Check the shapes of the outputs\n",
    "print(\"X_train: {}\".format(np.shape(X_train)))\n",
    "print(\"y_train: {}\".format(np.shape(y_train)))\n",
    "print(\"X_val: {}\".format(np.shape(X_val)))\n",
    "print(\"y_val: {}\".format(np.shape(y_val)))\n",
    "print(\"X_test: {}\".format(np.shape(X_test)))\n",
    "print(\"y_test: {}\".format(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the created arrays\n",
    "print(\"No. of patients: {}\".format(len(dataset)))\n",
    "print(\"No. of patients in train, val, test sets: {}, {}, {}\".format(len(X_train), len(X_val), len(X_test)))\n",
    "print(\"No. of patients who died in hospital in train, val, test sets: {}, {}, {}\".format(str(sum(y_train)), str(sum(y_val)), str(sum(y_test))))\n",
    "print(\"Proportion of patients who died in hospital in train, val, test sets: {}%, {}%, {}%\".format(round(sum(y_train)/len(y_train)*100,3), round(sum(y_val)/len(y_val)*100,3), round(sum(y_test)/len(y_test)*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to normalises the data using the MinMaxScaler, treating each feature separately\n",
    "def MinMaxScaler_3D(array, minimum=None, maximum=None):\n",
    "    \n",
    "    '''\n",
    "    INPUTS:\n",
    "    array - a 3D array of shape (m, n, T), where m = number of examples, n = number of features, and T = number of time steps.\n",
    "    minimum (optional) - the minimum you want to apply for the scaling. If not specified, the minimum will be calculated from the array.\n",
    "    maximum (optional) - the maximum you want to apply for the scaling. If not specified, the maximum will be calculated from the array.\n",
    "    -> note that minimum and maximum, if specified, need to be n-length vectors, where each entry represents the min/max for each feature in the array.\n",
    "    \n",
    "    OUTPUTS:\n",
    "    scaled_array - a 3D array where each entry in array has been scaled using the equation y = (x - min) / (max - min), and the min/max has been calculated individually for each feature.\n",
    "    min_vector (optional) - an n-length vector where each entry is the minimum for each feature. Only returned if minimum is not specified.\n",
    "    max_vector (optional) - an n-length vector where each entry is the maximum for each feature. Only returned if maximum is not specified.\n",
    "    '''\n",
    "    \n",
    "    # First, get the dimensions of the input array\n",
    "    m, n, T = np.shape(array)\n",
    "    \n",
    "    # Calculate the mininimum of each feature\n",
    "    if minimum == None:\n",
    "        min_vector = []\n",
    "        for i in range(n):\n",
    "            min_vector.append(np.min(array[:,i,:]))\n",
    "        no_minmax = True # used for deciding whether to output the min_vector at the end\n",
    "    else:\n",
    "        min_vector = minimum\n",
    "        no_minmax = False\n",
    "    \n",
    "    # Calculate the maximum of each feature\n",
    "    if maximum == None:\n",
    "        max_vector = []\n",
    "        for i in range(n):\n",
    "            max_vector.append(np.max(array[:,i,:]))\n",
    "    else:\n",
    "        max_vector = maximum\n",
    "        \n",
    "    # Scale each feature using the formula: y = (x - min) / (max - min)\n",
    "    scaled_array = np.zeros((m,n,T))\n",
    "    for i in range(n):\n",
    "        scaled_array[:,i,:] = (array[:,i,:] - min_vector[i]) / (max_vector[i] - min_vector[i])\n",
    "    \n",
    "    # Return the outputs\n",
    "    if no_minmax == True:\n",
    "        return scaled_array, min_vector, max_vector\n",
    "    else:\n",
    "        return scaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to normalises the data using the z-score, treating each feature separately\n",
    "def ZScoreScaler_3D(array, mean=None, std=None):\n",
    "    \n",
    "    '''\n",
    "    INPUTS:\n",
    "    array - a 3D array of shape (m, n, T), where m = number of examples, n = number of features, and T = number of time steps.\n",
    "    mean (optional) - the mean you want to apply for the scaling. If not specified, the mean will be calculated from the array.\n",
    "    std (optional) - the standard deviation you want to apply for the scaling. If not specified, the standard deviation will be calculated from the array.\n",
    "    -> note that minimum and maximum, if specified, need to be n-length vectors, where each entry represents the min/max for each feature in the array.\n",
    "    \n",
    "    OUTPUTS:\n",
    "    scaled_array - a 3D array where each entry in array has been scaled using the equation y = (x - mean) / std, and the mean/standard deviation has been calculated individually for each feature.\n",
    "    mean_vector (optional) - an n-length vector where each entry is the mean for each feature. Only returned if mean is not specified.\n",
    "    std_vector (optional) - an n-length vector where each entry is the standard deviation for each feature. Only returned if std is not specified.\n",
    "    '''\n",
    "    \n",
    "    # First, get the dimensions of the input array\n",
    "    m, n, T = np.shape(array)\n",
    "    \n",
    "    # Calculate the mininimum of each feature\n",
    "    if mean == None:\n",
    "        mean_vector = []\n",
    "        for i in range(n):\n",
    "            mean_vector.append(np.mean(array[:,i,:]))\n",
    "        no_mean_std = True # used for deciding whether to output the min_vector at the end\n",
    "    else:\n",
    "        mean_vector = mean\n",
    "        no_mean_std = False\n",
    "    \n",
    "    # Calculate the maximum of each feature\n",
    "    if std == None:\n",
    "        std_vector = []\n",
    "        for i in range(n):\n",
    "            std_vector.append(np.std(array[:,i,:]))\n",
    "    else:\n",
    "        std_vector = std\n",
    "        \n",
    "    # Scale each feature using the formula: y = (x - mean) / std\n",
    "    scaled_array = np.zeros((m,n,T))\n",
    "    for i in range(n):\n",
    "        scaled_array[:,i,:] = (array[:,i,:] - mean_vector[i]) / std_vector[i]\n",
    "    \n",
    "    # Return the outputs\n",
    "    if no_mean_std == True:\n",
    "        return scaled_array, mean_vector, std_vector\n",
    "    else:\n",
    "        return scaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training data, and save the scaler\n",
    "X_train_norm, mean_vector, std_vector = ZScoreScaler_3D(X_train)\n",
    "\n",
    "# Use the same scaler to scale the validation and test data\n",
    "X_val_norm = ZScoreScaler_3D(X_val, mean_vector, std_vector)\n",
    "X_test_norm = ZScoreScaler_3D(X_test, mean_vector, std_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the shapes, minimums and maximums of the scaled arrays just to make sure everything has worked OK\n",
    "print(\"X_train_norm:\")\n",
    "print(\"Shape: {}, minimum = {}, maximum = {}\".format(np.shape(X_train_norm), round(np.min(X_train_norm),2), round(np.max(X_train_norm),2)))\n",
    "print(\"\")\n",
    "print(\"X_val_norm:\")\n",
    "print(\"Shape: {}, minimum = {}, maximum = {}\".format(np.shape(X_val_norm), round(np.min(X_val_norm),2), round(np.max(X_val_norm),2)))\n",
    "print(\"\")\n",
    "print(\"X_test_norm:\")\n",
    "print(\"Shape: {}, minimum = {}, maximum = {}\".format(np.shape(X_test_norm), round(np.min(X_test_norm),2), round(np.max(X_test_norm),2)))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to data.npy\n",
    "if not os.path.exists('./res'):\n",
    "    os.makedirs('./res')\n",
    "\n",
    "tosave = {'X_train': X_train, 'X_val': X_val, 'X_test': X_test, , ...\n",
    "         'X_train_norm': X_train_norm, 'X_val_norm': X_val_norm, 'X_test': X_test_norm, ...\n",
    "         'y_train': y_train, 'y_val': y_val, 'y_test': y_test}\n",
    "np.save('res/data.npy',tosave)\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
